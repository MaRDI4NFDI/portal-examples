{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d37a4976-197a-4cc2-93cb-27d8c1083b60",
   "metadata": {},
   "source": [
    "# Filter papers by software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69e2769d-be70-4285-8965-5a06b6fd1599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load utilities shared between notebooks\n",
    "%run definitions_and_functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef34d8c-61d9-440a-a8de-c0b5fd59573b",
   "metadata": {},
   "source": [
    "There are millions of references to papers in the zbMath database. We just need (for now) those related to the list of mathematical software (data/swMATH-software-list.csv) that has been imported into the MaRDI-Portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7822c109-1516-4591-9126-4cead1fa6951",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the list of swMath software\n",
    "software_df = pd.read_csv('data/swMATH-software-list.csv')\n",
    "softwares = software_df['Len'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03766153-a8ce-4549-9ec1-c7323553b2ec",
   "metadata": {},
   "source": [
    "**I'm looking at these attributes for each paper, please double check if that's OK, since importing everything will take hours.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a30ff785-907f-4414-885c-4594ef9e0d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zbMath paper id and ['author', 'author_ids', 'document_title', 'source', 'classifications', 'keywords', 'doi', 'publication_year']\n"
     ]
    }
   ],
   "source": [
    "print('zbMath paper id and', TAGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f617c252-4b9a-41ce-a84f-817459470d47",
   "metadata": {},
   "source": [
    "## Get a list of records related to a single software\n",
    "Use the helper/filter endpoint to get a list of papers related to a particular software. In this case 'Gfan'.\n",
    "\n",
    "**Doesn't work for some software (e.g. FORTRAN) mailed OAI suport about this**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2910e2ac-dbf6-4d6b-be42-9f2e2bfa59a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "software = 'Gfan'\n",
    "REQUEST_URL=\"{}&filter=software:{}\".format(FILTER, software)\n",
    "\n",
    "# get data from API\n",
    "headers = {'accept': 'text/xml'} # this has no effect\n",
    "all_records_xml = requests.get(REQUEST_URL, headers)\n",
    "if all_records_xml.status_code == 200:\n",
    "    # save raw data in local file\n",
    "    with open('data/software_records_{}.xml'.format(software), 'w') as f:\n",
    "        f.write(all_records_xml.text)    \n",
    "else: \n",
    "    print(all_records_xml.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45426894-49d1-477a-ad50-dd521d89af8d",
   "metadata": {},
   "source": [
    "Define a function to parse all records in the data set, put them into a pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1212f48f-e985-4f3f-af5d-eace0b0afe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "def parse_record_list(records, verbose=False):\n",
    "    \"\"\"\n",
    "    Parses a list of 'record' XML elements.\n",
    "    @return pandas DataFrame or None (if no suitably licensed data was found)\n",
    "    \"\"\"\n",
    "    # loop through all entries\n",
    "    all_details = []\n",
    "    for record in records:\n",
    "        details = parse_record(record)\n",
    "        if details:\n",
    "            all_details.append(details)\n",
    "\n",
    "    # convert to data frame\n",
    "    records_df = pd.DataFrame(all_details)\n",
    "    if 'id' not in records_df.columns:\n",
    "        if verbose: print(\"Problem reading zbMath id's. No data suitably licensed data?\")\n",
    "        return None\n",
    "    else:\n",
    "        records_df.set_index('id', inplace=True)\n",
    "        if verbose:\n",
    "            print('Imported {} entries (discarded {} for licensing conflicts)'.format(len(records_df), len(records) - len(records_df) ))\n",
    "        \n",
    "    return records_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cfb724-26f2-4eca-8d25-0e6320c99eee",
   "metadata": {},
   "source": [
    "## Get records for the first 1000 software entries\n",
    "Go through the list of software, get the records from the zbMath Open API, put them in a data frame.\n",
    "\n",
    "Only process the first 1000 entries, as there are +- 40000 software entries in the list (processing the first 1000 entries took 5:34, so total would be +- 3h 40 min).\n",
    "\n",
    "If there's an API error, log and continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20f72871-5981-4dbe-a5ce-c0fe9bc9a949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 100/1000 entries\n",
      "processed 200/1000 entries\n",
      "processed 300/1000 entries\n",
      "processed 400/1000 entries\n",
      "processed 500/1000 entries\n",
      "processed 600/1000 entries\n",
      "processed 700/1000 entries\n",
      "processed 800/1000 entries\n",
      "processed 900/1000 entries\n",
      "Done. Check data/api.log for errors\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from xml.etree.ElementTree import ParseError\n",
    "\n",
    "MAX_ENTRIES = 1000 # number of entries to process\n",
    "all_details = pd.DataFrame() # the final data frame\n",
    "counter = 0\n",
    "for software in softwares[:MAX_ENTRIES]:\n",
    "    headers = {'accept': 'text/xml'} # this has no effect\n",
    "    REQUEST_URL=\"{}&filter=software:{}\".format(FILTER, software)\n",
    "    \n",
    "    try:\n",
    "        # get data from API\n",
    "        all_records_xml = requests.get(REQUEST_URL, headers)\n",
    "        if all_records_xml.status_code == 500: raise ZbMathOpenAPIException(all_records_xml.reason)\n",
    "        \n",
    "        #parse the tree, get a list of records\n",
    "        tree = ET.fromstring(all_records_xml.text)\n",
    "        list_ids = tree.find(ns('ListRecords')) # when parsing XML from string, don't call getroot()\n",
    "        records = list_ids.findall(ns('record'))\n",
    "        # parse record details into a data frame\n",
    "        records_df = parse_record_list(records)\n",
    "        \n",
    "        # add name of software and append new records to final data frame        \n",
    "        if records_df is not None:\n",
    "            records_df['software'] = software\n",
    "            all_details = all_details.append(records_df)\n",
    "            # log success\n",
    "            log('Processing {}: Imported {} entries (discarded {} for licensing conflicts)'.format(software, len(records_df), len(records) - len(records_df) ))\n",
    "        else:\n",
    "            # no suitable records found\n",
    "            log(\"[WARNING] Processing {}: Problem reading zbMath id's. No data suitably licensed data?\".format(software))\n",
    "            \n",
    "        # print progress feedback\n",
    "        counter += 1\n",
    "        if counter % 100 == 0: print(\"processed {}/{} entries\".format(counter, MAX_ENTRIES))\n",
    "            \n",
    "    except ZbMathOpenAPIException as e:\n",
    "        # if the API throws a server-side error, log and continue\n",
    "        log(\"[ERROR] While reading filter=software:{} from zbMath Open API help/filter endpoint: {}\".format(software, e))\n",
    "    except ParseError as e:\n",
    "        # if the API returns wrong XML, log and continue\n",
    "        log(\"[ERROR] While reading filter=software:{} from zbMath Open API help/filter endpoint: {}\".format(software, e))\n",
    "\n",
    "print(\"Done. Check {} for errors\".format(IMPORT_LOG))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b18c199-8f21-492f-b4a8-c75315ea0fd8",
   "metadata": {},
   "source": [
    "Save data in compressed file for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8d0614f8-a9bd-4f78-b674-83042edc72db",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_details.to_csv('data/all_records.csv.zip', compression=dict(method='zip', archive_name='all_records.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3154c29-d2cd-4c1b-a4b1-48889b5ca3a3",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "* drop incomplete entries: no author or no author ids or no title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "87352e81-4ac8-4641-8a80-d20a58b19742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data frame from file: rows 11577, columns 10\n",
      "981 incomplete entries were removed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# re-read the data from file\n",
    "all_details = pd.read_csv('data/software_all_records.csv.zip')\n",
    "print(\"Read data frame from file: rows {}, columns {}\".format(all_details.shape[0], all_details.shape[1]))\n",
    "\n",
    "# drop entries without an author or a title\n",
    "idx = (all_details.author.isna()) | all_details.author_ids.isna() | (all_details.document_title.isna())\n",
    "all_details = all_details[~idx]\n",
    "print('{} incomplete entries were removed'.format(idx.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e49bdb-b8db-49f9-be4d-67d1f8878a00",
   "metadata": {},
   "source": [
    "## Consolidate data\n",
    "Prepare data for import into wikibase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdf8887-6892-485c-a8ed-15b2de491311",
   "metadata": {},
   "source": [
    "### A data frame with all the papers\n",
    "Drop duplicates: same doi, same combination of authors-title-year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "66a5322b-e5a0-4621-944f-b9a564bf3f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9193 papers. 1403 duplicated entries were removed\n"
     ]
    }
   ],
   "source": [
    "all_papers = all_details.drop(columns='software')\n",
    "idx = (all_papers.duplicated(subset='doi')) | (all_papers.duplicated(subset=['author', 'document_title', 'publication_year']))\n",
    "all_papers = all_papers[~idx]\n",
    "print('Found {} papers. {} duplicated entries were removed'.format(len(all_papers), idx.sum()))\n",
    "all_papers.to_csv('data/all_papers.csv.zip', compression=dict(method='zip', archive_name='all_papers.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df6ef16-e57d-40c6-8b87-bef1bb5f6ce3",
   "metadata": {},
   "source": [
    "### A data frame with all the authors and author_ids\n",
    "All authors should be listed in a separate data frame to facilitate import into wikibase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7565a683-bde6-400c-a6a4-657405a635b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17985 authors\n"
     ]
    }
   ],
   "source": [
    "authors_df = pd.DataFrame(columns=['author_id', 'author_name'])\n",
    "for _,row in all_papers.iterrows():\n",
    "    author_ids = row.author_ids.split(';')\n",
    "    author_names = row.author.split(';')\n",
    "    for i in range(min(len(author_ids), len(author_names))):\n",
    "        authors_df = authors_df.append({'author_id': author_ids[i], 'author_name': author_names[i]}, ignore_index=True)\n",
    "\n",
    "# remove duplicates\n",
    "idx = authors_df.duplicated()\n",
    "authors_df = authors_df[~idx]\n",
    "authors_df = authors_df.set_index('author_id').sort_index()\n",
    "print('Found {} authors'.format(len(authors_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "17d1a509-633d-402c-9090-7e0f3194efdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df.to_csv('data/all_authors.csv.zip', compression=dict(method='zip', archive_name='all_authors.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771649c8-55b3-4383-b9d2-035503e56760",
   "metadata": {},
   "source": [
    "### A data frame with all the paper-to-software relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a1d06219-893c-4ca7-8dc3-f5567ab8cf3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10596 paper-to-software relations\n"
     ]
    }
   ],
   "source": [
    "diff = all_details.columns.difference(['id', 'software'])\n",
    "all_papers_software = all_details.drop(columns=diff)\n",
    "print(\"Found {} paper-to-software relations\".format(len(all_papers_software)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1fc1985f-e3d2-41c4-862d-5f43613b96fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers_software.to_csv('data/all_papers_software.csv.zip', compression=dict(method='zip', archive_name='all_papers_software.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512fabb8-6e9e-48cd-8fa6-ad9c26da3af7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
