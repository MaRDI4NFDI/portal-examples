{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f946d5b-a794-4461-9400-711a2ef61ebf",
   "metadata": {},
   "source": [
    "# Import properties from Wikidata\n",
    "\n",
    "Used [Protege online](https://webprotege.stanford.edu/) to model the required classes and properties.\n",
    "\n",
    "The resulting model was saved in OWL/XMLformat (`data/knowledge-graph.owl`).\n",
    "\n",
    "Used [WebVOWL](http://vowl.visualdataweb.org/webvowl.html) to visualize the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf4693-b5eb-4685-8365-476a1d3fc954",
   "metadata": {},
   "source": [
    "![title](data/knowledge-graph.owl.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e91f106-2abd-47f3-909f-28f4a30a98ce",
   "metadata": {},
   "source": [
    "1. Start a local wikibase, e.g. using [MaRDI4NFDI/portal-compose](https://github.com/MaRDI4NFDI/portal-compose)\n",
    "2. Increase memory limit by setting `ini_set( 'memory_limit', '2G' );` in LocalSettings.d/LocalSettings.override.php\n",
    "\n",
    "Possible ways to import items and properties from Wikidata (see https://www.mediawiki.org/wiki/Wikibase/Importing):\n",
    "* ~Enable federated properties from Wikidata by setting `$wgWBRepoSettings['federatedPropertiesEnabled'] = true;` in Localsettings.d/LocalSettings.override.php~  (potentially creates entity namimg conflicts)\n",
    "* ~Use native MediaWiki export/import to load the required pages fom Wikidata by importing the XML files into the local wikibase. These files contains item and property pages that have been exported from Wikidata.~ (potentially creates entity naming conflicts)\n",
    "* Use WikibaseImport to import pages from Wikidata. ([older version](https://github.com/Wikidata/WikibaseImport/tree/c1233da6b7122c55c95d9d925b8f4162de8807e7) works, but I had to fork it, see below)\n",
    "* ~Use a custom script to import the XML dumps from Wikidata into the local wiki without overwritting existing items and properties~ (last resort, as more work)\n",
    "\n",
    "_Properties and items to import are listed in the file `data/Wikidata_import.csv`._\n",
    "\n",
    "__Property pages that need to be imported from Wikidata__\n",
    "* instance of (Property:P31)\n",
    "* zbMATH author ID (Property:P1556)\n",
    "* zbMATH work ID (Property:P894) \n",
    "* swMATH work ID (Property:P6830) \n",
    "* formatter URL (Property:P1630)\n",
    "* DOI (Property:P356)\n",
    "* describes a project that uses (Property:P4510)\n",
    "* author (Property:P50)\n",
    "* publication date (Property:P577)\n",
    "* published in (Property:P1433) \n",
    "* publisher (Property:P123)\n",
    "* title (Property:P1476)\n",
    "* main subject (Property:P921) \n",
    "* language of work or name (Property:P407)\n",
    "* volume (Property:P478)\n",
    "* page(s) (Property:P304)\n",
    "* issue (Property:P433)\n",
    "* exact match (Property:P2888)\n",
    "\n",
    "__Item pages that need to be imported from Wikidata__\n",
    "* human (Q5)\n",
    "* software (Q7397)\n",
    "* scholarly article (Q13442814) \n",
    "* scientific publication (Q591041)\n",
    "* publisher (Q2085381)\n",
    "* index term (Q1128340) \n",
    "* language (Q34770)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c83d89-d9a4-4151-975f-bf68ff80a8c3",
   "metadata": {},
   "source": [
    "# Importing using WikiImport\n",
    "The properties and items to be imported have to be in a text file `data/Wikidata_import.csv`.\n",
    "Since the import script will import not only the items listed in the file, but also all referenced properties and items, the list of imports might be quite large and take a while to complete. I [forked WikibaseImport](https://github.com/MaRDI4NFDI/WikibaseImport) to not import recursively, but only the items that are in the list.\n",
    "\n",
    "## Try it out\n",
    "To test the import manually, copy the list to the running Wikibase container:\n",
    "```\n",
    "docker cp Import\\ from\\ zbMath/data/Wikidata_import.csv mardi-wikibase:/tmp\n",
    "```\n",
    "\n",
    "Then run the import script. This script is a Wikibase extension, so runs from the Wikibase container:\n",
    "```\n",
    "docker exec -ti mardi-wikibase php extensions/WikibaseImport/maintenance/importEntities.php --file /tmp/Wikidata_import.csv --do-not-recurse\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87706806-0800-4067-ae2f-4021fe4e9779",
   "metadata": {},
   "source": [
    "## Entity mapping\n",
    "The properties and items imported from Wikidata are mapped to local ids. The mapping between original Wikidata ids and local ids is stored in the wiki's database in an extra table `wbs_entity_mapping`. For example:\n",
    "\n",
    "```\n",
    "MariaDB [my_wiki]> select * from wbs_entity_mapping limit 4;\n",
    "+--------------+-----------------+\n",
    "| wbs_local_id | wbs_original_id |\n",
    "+--------------+-----------------+\n",
    "| P76          | P10             |\n",
    "| P119         | P1001           |\n",
    "| P113         | P101            |\n",
    "| P122         | P1011           |\n",
    "+--------------+-----------------+\n",
    "```\n",
    "\n",
    "It would also be possible to add this mapping as a property to each entity page in the local Wikibase, but this should be backed by a specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27033003-986e-46da-8543-4a7f669aff65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
